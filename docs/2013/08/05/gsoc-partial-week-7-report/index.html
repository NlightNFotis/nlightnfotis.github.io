<!DOCTYPE html>
<html lang="en-GB">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>GSOC (Partial) Week 7 report</title>
  <meta name="description" content="An exciting week. This week was exciting. Spending it on learning about the go runtime was the reason for this. As insightfull as it was however, it also confused me a little bit. Before this goes any further, I should state that this is a partial report on my research and my findings. My aims for this week were the following: To investigate the behavior of go programs under the Hurd, to study the go runtime, and possibly modify it to see if the goroutine issues are libpthread’s issue or the go’s runtime issue. Presenting my findings. Most of my time was spent studying the gcc go frontend, libgo and the go runtime. Fortunatelly, I can say (gladly) that it was time well spent. What I got from it were some nice pieces of insight, but also some slight confusion and doubts. The first interesting thing in my findings was this: struct G { Defer* defer; Panic* panic; void* exception; // current exception being thrown bool is_foreign; // whether current exception from other language void *gcstack; // if status==Gsyscall, gcstack = stackbase to use during gc uintptr gcstack_size; void* gcnext_segment; void* gcnext_sp; void* gcinitial_sp; ucontext_t gcregs; byte* entry; // initial function G* alllink; // on allg void* param; // passed parameter on wakeup bool fromgogo; // reached from gogo int16 status; int64 goid; uint32 selgen; // valid sudog pointer const char* waitreason; // if status==Gwaiting G* schedlink; bool readyonstop; bool ispanic; bool issystem; int8 raceignore; // ignore race detection events M* m; // for debuggers, but offset not hard-coded M* lockedm; M* idlem; int32 sig; int32 writenbuf; byte* writebuf; // DeferChunk *dchunk; // DeferChunk *dchunknext; uintptr sigcode0; uintptr sigcode1; // uintptr sigpc; uintptr gopc; // pc of go statement that created this goroutine int32 ncgo; CgoMal* cgomal; Traceback* traceback; ucontext_t context; void* stack_context[10]; }; Yep. This is the code that resembles a (yeah, you guessed it, a goroutine). I was pretty surprised at first to see that a thread is resembled as a struct. But then again, taking a closer look at it, it makes perfect sense. The next one though was a lot trickier: struct M { G* g0; // goroutine with scheduling stack G* gsignal; // signal-handling G G* curg; // current running goroutine int32 id; int32 mallocing; int32 throwing; int32 gcing; int32 locks; int32 nomemprof; int32 waitnextg; int32 dying; int32 profilehz; int32 helpgc; uint32 fastrand; uint64 ncgocall; // number of cgo calls in total Note havenextg; G* nextg; M* alllink; // on allm M* schedlink; MCache *mcache; G* lockedg; G* idleg; Location createstack[32]; // Stack that created this thread. M* nextwaitm; // next M waiting for lock uintptr waitsema; // semaphore for parking on locks uint32 waitsemacount; uint32 waitsemalock; GCStats gcstats; bool racecall; void* racepc; uintptr settype_buf[1024]; uintptr settype_bufsize; uintptr end[]; }; This was a source of endless confusion at the beginning. It does have some hints reassuring the fact that G’s are indeed goroutines, but nothing that really helps to describe what an M is. It’s structure is identical to that of the G however, which means that it might have something to do with a thread. And indeed it is. Further study of the source code made me speculate that M’s must be the real operating system scheduled (kernel) threads, while G’s (goroutines) must be the lightweight threads managed by the go runtime. I was more than happy to find comments that reassured that position of mine. // The go scheduler&#39;s job is to match ready-to-run goroutines (`g&#39;s) // with waiting-for-work schedulers (`m&#39;s) Another cool finding was the go (runtime) scheduler - from which the above comment originates: struct Sched { Lock; G *gfree; // available g&#39;s (status == Gdead) int64 goidgen; G *ghead; // g&#39;s waiting to run G *gtail; int32 gwait; // number of g&#39;s waiting to run int32 gcount; // number of g&#39;s that are alive int32 grunning; // number of g&#39;s running on cpu or in syscall M *mhead; // m&#39;s waiting for work int32 mwait; // number of m&#39;s waiting for work int32 mcount; // number of m&#39;s that have been created volatile uint32 atomic; // atomic scheduling word (see below) int32 profilehz; // cpu profiling rate bool init; // running initialization bool lockmain; // init called runtime.LockOSThread Note stopped; // one g can set waitstop and wait here for m&#39;s to stop }; From that particular piece of code, without a doubt the most interesting line is: G *gfree. That is a pool of the go routines that are available to be used. There are also helper schedulling functions, from which, the most interesting (for my purposes), was the static void gfput(G*); which realeases a go routine (puts it to the gfree list) // Put on gfree list. Sched must be locked. static void gfput(G *gp) { gp-&amp;gt;schedlink = runtime_sched.gfree; runtime_sched.gfree = gp; } There are loads of other extremely interesting functions there, but for the sake of space I will not expand here more. However I will expand on what it is that is confusing me: The source of confusion My tests in this point are to include testing if removing thread destruction from the go runtime would result in difference in behavior. There are however (as far as go is concerned), two kinds of threads in the go runtime. Goroutines (G’s) and the kernel schedulable threads (M’s). Neither of which, seem to really be destroyed. From my understanding so far, G’s are never totally destroyed (I may be wrong here, I am still researching this bit). Whenever they are about to “destroyed”, they are added to the scheduler’s list of freeG’s to allow for reuse, as evidenced by the gfput and gfget functions. M’s on the other hand (the kernel threads), also seem to not be destroyed. A comment in go’s scheduler seems to support this (// For now, m&#39;s never go away.) and as a matter of fact I could not find any code that destroyed M’s (I am still researching this bit). Since none of the two actually get destroyed, and seeing as thread creation alone should not be buggy, how come we are facing the specific bugs we are facing? I will try to provide with an interpretation: Either I am fairly wrong and M’s (or G’s or both) actually do get destroyed somewhere (possible and very much probable) or I looking for clues regarding the issue in the wrong place (might be possible but I don’t see it being very probable).">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://nlightnfotis.github.io/2013/08/05/gsoc-partial-week-7-report/">
  
  
  <link rel="alternate" type="application/rss+xml" title="Lambda reflections" href="https://nlightnfotis.github.io/feed.xml">

  

  
  <meta property="og:title" content="GSOC (Partial) Week 7 report">
  <meta property="og:site_name" content="Lambda reflections">
  <meta property="og:url" content="https://nlightnfotis.github.io/2013/08/05/gsoc-partial-week-7-report/">
  <meta property="og:description" content="An exciting week. This week was exciting. Spending it on learning about the go runtime was the reason for this. As insightfull as it was however, it also confused me a little bit. Before this goes any further, I should state that this is a partial report on my research and my findings. My aims for this week were the following: To investigate the behavior of go programs under the Hurd, to study the go runtime, and possibly modify it to see if the goroutine issues are libpthread’s issue or the go’s runtime issue. Presenting my findings. Most of my time was spent studying the gcc go frontend, libgo and the go runtime. Fortunatelly, I can say (gladly) that it was time well spent. What I got from it were some nice pieces of insight, but also some slight confusion and doubts. The first interesting thing in my findings was this: struct G { Defer* defer; Panic* panic; void* exception; // current exception being thrown bool is_foreign; // whether current exception from other language void *gcstack; // if status==Gsyscall, gcstack = stackbase to use during gc uintptr gcstack_size; void* gcnext_segment; void* gcnext_sp; void* gcinitial_sp; ucontext_t gcregs; byte* entry; // initial function G* alllink; // on allg void* param; // passed parameter on wakeup bool fromgogo; // reached from gogo int16 status; int64 goid; uint32 selgen; // valid sudog pointer const char* waitreason; // if status==Gwaiting G* schedlink; bool readyonstop; bool ispanic; bool issystem; int8 raceignore; // ignore race detection events M* m; // for debuggers, but offset not hard-coded M* lockedm; M* idlem; int32 sig; int32 writenbuf; byte* writebuf; // DeferChunk *dchunk; // DeferChunk *dchunknext; uintptr sigcode0; uintptr sigcode1; // uintptr sigpc; uintptr gopc; // pc of go statement that created this goroutine int32 ncgo; CgoMal* cgomal; Traceback* traceback; ucontext_t context; void* stack_context[10]; }; Yep. This is the code that resembles a (yeah, you guessed it, a goroutine). I was pretty surprised at first to see that a thread is resembled as a struct. But then again, taking a closer look at it, it makes perfect sense. The next one though was a lot trickier: struct M { G* g0; // goroutine with scheduling stack G* gsignal; // signal-handling G G* curg; // current running goroutine int32 id; int32 mallocing; int32 throwing; int32 gcing; int32 locks; int32 nomemprof; int32 waitnextg; int32 dying; int32 profilehz; int32 helpgc; uint32 fastrand; uint64 ncgocall; // number of cgo calls in total Note havenextg; G* nextg; M* alllink; // on allm M* schedlink; MCache *mcache; G* lockedg; G* idleg; Location createstack[32]; // Stack that created this thread. M* nextwaitm; // next M waiting for lock uintptr waitsema; // semaphore for parking on locks uint32 waitsemacount; uint32 waitsemalock; GCStats gcstats; bool racecall; void* racepc; uintptr settype_buf[1024]; uintptr settype_bufsize; uintptr end[]; }; This was a source of endless confusion at the beginning. It does have some hints reassuring the fact that G’s are indeed goroutines, but nothing that really helps to describe what an M is. It’s structure is identical to that of the G however, which means that it might have something to do with a thread. And indeed it is. Further study of the source code made me speculate that M’s must be the real operating system scheduled (kernel) threads, while G’s (goroutines) must be the lightweight threads managed by the go runtime. I was more than happy to find comments that reassured that position of mine. // The go scheduler&#39;s job is to match ready-to-run goroutines (`g&#39;s) // with waiting-for-work schedulers (`m&#39;s) Another cool finding was the go (runtime) scheduler - from which the above comment originates: struct Sched { Lock; G *gfree; // available g&#39;s (status == Gdead) int64 goidgen; G *ghead; // g&#39;s waiting to run G *gtail; int32 gwait; // number of g&#39;s waiting to run int32 gcount; // number of g&#39;s that are alive int32 grunning; // number of g&#39;s running on cpu or in syscall M *mhead; // m&#39;s waiting for work int32 mwait; // number of m&#39;s waiting for work int32 mcount; // number of m&#39;s that have been created volatile uint32 atomic; // atomic scheduling word (see below) int32 profilehz; // cpu profiling rate bool init; // running initialization bool lockmain; // init called runtime.LockOSThread Note stopped; // one g can set waitstop and wait here for m&#39;s to stop }; From that particular piece of code, without a doubt the most interesting line is: G *gfree. That is a pool of the go routines that are available to be used. There are also helper schedulling functions, from which, the most interesting (for my purposes), was the static void gfput(G*); which realeases a go routine (puts it to the gfree list) // Put on gfree list. Sched must be locked. static void gfput(G *gp) { gp-&amp;gt;schedlink = runtime_sched.gfree; runtime_sched.gfree = gp; } There are loads of other extremely interesting functions there, but for the sake of space I will not expand here more. However I will expand on what it is that is confusing me: The source of confusion My tests in this point are to include testing if removing thread destruction from the go runtime would result in difference in behavior. There are however (as far as go is concerned), two kinds of threads in the go runtime. Goroutines (G’s) and the kernel schedulable threads (M’s). Neither of which, seem to really be destroyed. From my understanding so far, G’s are never totally destroyed (I may be wrong here, I am still researching this bit). Whenever they are about to “destroyed”, they are added to the scheduler’s list of freeG’s to allow for reuse, as evidenced by the gfput and gfget functions. M’s on the other hand (the kernel threads), also seem to not be destroyed. A comment in go’s scheduler seems to support this (// For now, m&#39;s never go away.) and as a matter of fact I could not find any code that destroyed M’s (I am still researching this bit). Since none of the two actually get destroyed, and seeing as thread creation alone should not be buggy, how come we are facing the specific bugs we are facing? I will try to provide with an interpretation: Either I am fairly wrong and M’s (or G’s or both) actually do get destroyed somewhere (possible and very much probable) or I looking for clues regarding the issue in the wrong place (might be possible but I don’t see it being very probable).">
  
  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="NlightNFotis">
  <meta name="twitter:title" content="GSOC (Partial) Week 7 report">
  <meta name="twitter:description" content="An exciting week. This week was exciting. Spending it on learning about the go runtime was the reason for this. As insightfull as it was however, it also confused me a little bit. Before this goes ...">
  
    <meta name="twitter:creator" content="NlightNFotis">
  
  

  <link rel="dns-prefetch" href="https://fonts.gstatic.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Bitter:ital,wght@0,400;0,700;1,400&amp;display=swap" rel="stylesheet">

  

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Lambda reflections</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/">About</a>
      
        
        <a class="page-link" href="/archives/">Archives</a>
      
        
        <a class="page-link" href="https://github.com/NlightNFotis">GitHub</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    
      <h1 class="post-title" itemprop="name headline">GSOC (Partial) Week 7 report</h1>
    
    <p class="post-meta"><time datetime="2013-08-05T01:36:00+00:00" itemprop="datePublished">Aug 5, 2013</time> •
  
    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        <a href="/categories/gsoc/">gsoc</a>,
      
    
      
    
      
    
  
    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        <a href="/categories/golang/">golang</a>,
      
    
      
    
  
    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        <a href="/categories/gcc/">gcc</a>
      
    
  



</p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <h1>An exciting week.</h1>

<p>This week was exciting. Spending it on learning about the go runtime was the reason for this. As insightfull as it was however,
it also confused me a little bit. Before this goes any further, I should state that this is a partial report on my research
and my findings. My aims for this week were the following: <strong>To investigate the behavior of go programs under the Hurd, to
study the go runtime, and possibly modify it to see if the goroutine issues are libpthread’s issue or the go’s runtime issue</strong>.</p>

<h1>Presenting my findings.</h1>

<p>Most of my time was spent studying the gcc go frontend, libgo and the go runtime. Fortunatelly, I can say (gladly) that it was
time well spent. What I got from it were some nice pieces of insight, but also some slight confusion and doubts.</p>

<p>The first interesting thing in my findings was this:</p>

<figure class="highlight"><pre><code class="language-c" data-lang="c"><span class="k">struct</span>	<span class="n">G</span>
<span class="p">{</span>
	<span class="n">Defer</span><span class="o">*</span>	<span class="n">defer</span><span class="p">;</span>
	<span class="n">Panic</span><span class="o">*</span>	<span class="n">panic</span><span class="p">;</span>
	<span class="kt">void</span><span class="o">*</span>	<span class="n">exception</span><span class="p">;</span>	<span class="c1">// current exception being thrown</span>
	<span class="n">bool</span>	<span class="n">is_foreign</span><span class="p">;</span>	<span class="c1">// whether current exception from other language</span>
	<span class="kt">void</span>	<span class="o">*</span><span class="n">gcstack</span><span class="p">;</span>	<span class="c1">// if status==Gsyscall, gcstack = stackbase to use during gc</span>
	<span class="n">uintptr</span>	<span class="n">gcstack_size</span><span class="p">;</span>
	<span class="kt">void</span><span class="o">*</span>	<span class="n">gcnext_segment</span><span class="p">;</span>
	<span class="kt">void</span><span class="o">*</span>	<span class="n">gcnext_sp</span><span class="p">;</span>
	<span class="kt">void</span><span class="o">*</span>	<span class="n">gcinitial_sp</span><span class="p">;</span>
	<span class="n">ucontext_t</span> <span class="n">gcregs</span><span class="p">;</span>
	<span class="n">byte</span><span class="o">*</span>	<span class="n">entry</span><span class="p">;</span>		<span class="c1">// initial function</span>
	<span class="n">G</span><span class="o">*</span>	<span class="n">alllink</span><span class="p">;</span>	<span class="c1">// on allg</span>
	<span class="kt">void</span><span class="o">*</span>	<span class="n">param</span><span class="p">;</span>		<span class="c1">// passed parameter on wakeup</span>
	<span class="n">bool</span>	<span class="n">fromgogo</span><span class="p">;</span>	<span class="c1">// reached from gogo</span>
	<span class="n">int16</span>	<span class="n">status</span><span class="p">;</span>
	<span class="n">int64</span>	<span class="n">goid</span><span class="p">;</span>
	<span class="n">uint32</span>	<span class="n">selgen</span><span class="p">;</span>		<span class="c1">// valid sudog pointer</span>
	<span class="k">const</span> <span class="kt">char</span><span class="o">*</span>	<span class="n">waitreason</span><span class="p">;</span>	<span class="c1">// if status==Gwaiting</span>
	<span class="n">G</span><span class="o">*</span>	<span class="n">schedlink</span><span class="p">;</span>
	<span class="n">bool</span>	<span class="n">readyonstop</span><span class="p">;</span>
	<span class="n">bool</span>	<span class="n">ispanic</span><span class="p">;</span>
	<span class="n">bool</span>	<span class="n">issystem</span><span class="p">;</span>
	<span class="n">int8</span>	<span class="n">raceignore</span><span class="p">;</span> <span class="c1">// ignore race detection events</span>
	<span class="n">M</span><span class="o">*</span>	<span class="n">m</span><span class="p">;</span>		<span class="c1">// for debuggers, but offset not hard-coded</span>
	<span class="n">M</span><span class="o">*</span>	<span class="n">lockedm</span><span class="p">;</span>
	<span class="n">M</span><span class="o">*</span>	<span class="n">idlem</span><span class="p">;</span>
	<span class="n">int32</span>	<span class="n">sig</span><span class="p">;</span>
	<span class="n">int32</span>	<span class="n">writenbuf</span><span class="p">;</span>
	<span class="n">byte</span><span class="o">*</span>	<span class="n">writebuf</span><span class="p">;</span>
	<span class="c1">// DeferChunk	*dchunk;</span>
	<span class="c1">// DeferChunk	*dchunknext;</span>
	<span class="n">uintptr</span>	<span class="n">sigcode0</span><span class="p">;</span>
	<span class="n">uintptr</span>	<span class="n">sigcode1</span><span class="p">;</span>
	<span class="c1">// uintptr	sigpc;</span>
	<span class="n">uintptr</span>	<span class="n">gopc</span><span class="p">;</span>	<span class="c1">// pc of go statement that created this goroutine</span>

	<span class="n">int32</span>	<span class="n">ncgo</span><span class="p">;</span>
	<span class="n">CgoMal</span><span class="o">*</span>	<span class="n">cgomal</span><span class="p">;</span>

	<span class="n">Traceback</span><span class="o">*</span> <span class="n">traceback</span><span class="p">;</span>

	<span class="n">ucontext_t</span>	<span class="n">context</span><span class="p">;</span>
	<span class="kt">void</span><span class="o">*</span>		<span class="n">stack_context</span><span class="p">[</span><span class="mi">10</span><span class="p">];</span>
<span class="p">};</span></code></pre></figure>

<p>Yep. This is the code that resembles a (yeah, you guessed it, a <strong>goroutine</strong>). I was pretty surprised at first to see that a thread is resembled as a struct. But then again,
taking a closer look at it, it makes perfect sense. The next one though was a <em>lot trickier</em>:</p>

<figure class="highlight"><pre><code class="language-c" data-lang="c"><span class="k">struct</span>	<span class="n">M</span>
<span class="p">{</span>
	<span class="n">G</span><span class="o">*</span>	<span class="n">g0</span><span class="p">;</span>		<span class="c1">// goroutine with scheduling stack</span>
	<span class="n">G</span><span class="o">*</span>	<span class="n">gsignal</span><span class="p">;</span>	<span class="c1">// signal-handling G</span>
	<span class="n">G</span><span class="o">*</span>	<span class="n">curg</span><span class="p">;</span>		<span class="c1">// current running goroutine</span>
	<span class="n">int32</span>	<span class="n">id</span><span class="p">;</span>
	<span class="n">int32</span>	<span class="n">mallocing</span><span class="p">;</span>
	<span class="n">int32</span>	<span class="n">throwing</span><span class="p">;</span>
	<span class="n">int32</span>	<span class="n">gcing</span><span class="p">;</span>
	<span class="n">int32</span>	<span class="n">locks</span><span class="p">;</span>
	<span class="n">int32</span>	<span class="n">nomemprof</span><span class="p">;</span>
	<span class="n">int32</span>	<span class="n">waitnextg</span><span class="p">;</span>
	<span class="n">int32</span>	<span class="n">dying</span><span class="p">;</span>
	<span class="n">int32</span>	<span class="n">profilehz</span><span class="p">;</span>
	<span class="n">int32</span>	<span class="n">helpgc</span><span class="p">;</span>
	<span class="n">uint32</span>	<span class="n">fastrand</span><span class="p">;</span>
	<span class="n">uint64</span>	<span class="n">ncgocall</span><span class="p">;</span>	<span class="c1">// number of cgo calls in total</span>
	<span class="n">Note</span>	<span class="n">havenextg</span><span class="p">;</span>
	<span class="n">G</span><span class="o">*</span>	<span class="n">nextg</span><span class="p">;</span>
	<span class="n">M</span><span class="o">*</span>	<span class="n">alllink</span><span class="p">;</span>	<span class="c1">// on allm</span>
	<span class="n">M</span><span class="o">*</span>	<span class="n">schedlink</span><span class="p">;</span>
	<span class="n">MCache</span>	<span class="o">*</span><span class="n">mcache</span><span class="p">;</span>
	<span class="n">G</span><span class="o">*</span>	<span class="n">lockedg</span><span class="p">;</span>
	<span class="n">G</span><span class="o">*</span>	<span class="n">idleg</span><span class="p">;</span>
	<span class="n">Location</span> <span class="n">createstack</span><span class="p">[</span><span class="mi">32</span><span class="p">];</span>	<span class="c1">// Stack that created this thread.</span>
	<span class="n">M</span><span class="o">*</span>	<span class="n">nextwaitm</span><span class="p">;</span>	<span class="c1">// next M waiting for lock</span>
	<span class="n">uintptr</span>	<span class="n">waitsema</span><span class="p">;</span>	<span class="c1">// semaphore for parking on locks</span>
	<span class="n">uint32</span>	<span class="n">waitsemacount</span><span class="p">;</span>
	<span class="n">uint32</span>	<span class="n">waitsemalock</span><span class="p">;</span>
	<span class="n">GCStats</span>	<span class="n">gcstats</span><span class="p">;</span>
	<span class="n">bool</span>	<span class="n">racecall</span><span class="p">;</span>
	<span class="kt">void</span><span class="o">*</span>	<span class="n">racepc</span><span class="p">;</span>

	<span class="n">uintptr</span>	<span class="n">settype_buf</span><span class="p">[</span><span class="mi">1024</span><span class="p">];</span>
	<span class="n">uintptr</span>	<span class="n">settype_bufsize</span><span class="p">;</span>

	<span class="n">uintptr</span>	<span class="n">end</span><span class="p">[];</span>
<span class="p">};</span></code></pre></figure>

<p>This was a source of endless confusion at the beginning. It does have some hints reassuring the fact that G’s are indeed goroutines, but nothing that really helps to describe what an M is.
It’s structure is identical to that of the G however, which means that it might have something to do with a thread. And indeed it is. Further study of the source code
made me speculate that <strong>M’s must be the real operating system scheduled (kernel) threads, while G’s (goroutines) must be the lightweight threads managed by the go runtime.</strong></p>

<p>I was more than happy to find comments that reassured that position of mine.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// The go scheduler's job is to match ready-to-run goroutines (`g's)
// with waiting-for-work schedulers (`m's)
</code></pre></div></div>

<p>Another cool finding was the go (runtime) scheduler - from which the above comment originates:</p>

<figure class="highlight"><pre><code class="language-c" data-lang="c"><span class="k">struct</span> <span class="n">Sched</span> <span class="p">{</span>
	<span class="n">Lock</span><span class="p">;</span>

	<span class="n">G</span> <span class="o">*</span><span class="n">gfree</span><span class="p">;</span>	<span class="c1">// available g's (status == Gdead)</span>
	<span class="n">int64</span> <span class="n">goidgen</span><span class="p">;</span>

	<span class="n">G</span> <span class="o">*</span><span class="n">ghead</span><span class="p">;</span>	<span class="c1">// g's waiting to run</span>
	<span class="n">G</span> <span class="o">*</span><span class="n">gtail</span><span class="p">;</span>
	<span class="n">int32</span> <span class="n">gwait</span><span class="p">;</span>	<span class="c1">// number of g's waiting to run</span>
	<span class="n">int32</span> <span class="n">gcount</span><span class="p">;</span>	<span class="c1">// number of g's that are alive</span>
	<span class="n">int32</span> <span class="n">grunning</span><span class="p">;</span>	<span class="c1">// number of g's running on cpu or in syscall</span>

	<span class="n">M</span> <span class="o">*</span><span class="n">mhead</span><span class="p">;</span>	<span class="c1">// m's waiting for work</span>
	<span class="n">int32</span> <span class="n">mwait</span><span class="p">;</span>	<span class="c1">// number of m's waiting for work</span>
	<span class="n">int32</span> <span class="n">mcount</span><span class="p">;</span>	<span class="c1">// number of m's that have been created</span>

	<span class="k">volatile</span> <span class="n">uint32</span> <span class="n">atomic</span><span class="p">;</span>	<span class="c1">// atomic scheduling word (see below)</span>

	<span class="n">int32</span> <span class="n">profilehz</span><span class="p">;</span>	<span class="c1">// cpu profiling rate</span>

	<span class="n">bool</span> <span class="n">init</span><span class="p">;</span>  <span class="c1">// running initialization</span>
	<span class="n">bool</span> <span class="n">lockmain</span><span class="p">;</span>  <span class="c1">// init called runtime.LockOSThread</span>

	<span class="n">Note</span>	<span class="n">stopped</span><span class="p">;</span>	<span class="c1">// one g can set waitstop and wait here for m's to stop</span>
<span class="p">};</span></code></pre></figure>

<p>From that particular piece of code, without a doubt the most interesting line is: <code class="language-plaintext highlighter-rouge">G *gfree</code>. That is a pool of the go routines that are available to be used.
There are also helper schedulling functions, from which, the most interesting (for my purposes), was the <code class="language-plaintext highlighter-rouge">static void gfput(G*);</code> which realeases a go routine (puts it to the gfree list)</p>

<figure class="highlight"><pre><code class="language-c" data-lang="c"><span class="c1">// Put on gfree list.  Sched must be locked.</span>
<span class="k">static</span> <span class="kt">void</span>
<span class="nf">gfput</span><span class="p">(</span><span class="n">G</span> <span class="o">*</span><span class="n">gp</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">gp</span><span class="o">-&gt;</span><span class="n">schedlink</span> <span class="o">=</span> <span class="n">runtime_sched</span><span class="p">.</span><span class="n">gfree</span><span class="p">;</span>
	<span class="n">runtime_sched</span><span class="p">.</span><span class="n">gfree</span> <span class="o">=</span> <span class="n">gp</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>

<p>There are loads of other extremely interesting functions there, but for the sake of space I will not expand here more. However I will expand on what it is that is confusing me:</p>

<h2>The source of confusion</h2>

<p>My tests in this point are to include testing if removing thread destruction from the go runtime would result in difference in behavior.
There are however (as far as go is concerned), two kinds of threads in the go runtime. <strong>Goroutines</strong> (G’s) and the <strong>kernel schedulable threads</strong> (M’s).</p>

<p>Neither of which, seem to really be destroyed. From my understanding so far, G’s are never totally destroyed (I may be wrong here, I am still researching this bit). Whenever
they are about to “destroyed”, they are added to the scheduler’s list of freeG’s to allow for reuse, as evidenced by the <code class="language-plaintext highlighter-rouge">gfput</code> and <code class="language-plaintext highlighter-rouge">gfget</code> functions. 
M’s on the other hand (the kernel threads), also seem to not be destroyed. A comment in go’s scheduler seems to support this (<code class="language-plaintext highlighter-rouge">// For now, m's never go away.</code>) and as a 
matter of fact I could not find any code that destroyed M’s (I am still researching this bit).</p>

<p><strong>Since none of the two actually get destroyed, and seeing as thread creation alone should not be buggy, how come we are facing the specific bugs we are facing?</strong>
I will try to provide with an interpretation: Either I am fairly wrong and M’s (or G’s or both) actually do get destroyed somewhere (possible and very much probable)
or I looking for clues regarding the issue in the wrong place (might be possible but I don’t see it being very probable).</p>

  </div>

  

</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

&copy; Fotis Koutoulakis - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="https://nlightnfotis.github.io/feed.xml">RSS</a>

    </p>

  </div>

</footer>


  </body>

</html>
